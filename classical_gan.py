"""
classical_gan.py

Professional Classical GAN Implementation for Geometric Design Generation
========================================================================

A robust, CUDA-accelerated classical Generative Adversarial Network system
for creating geometric designs with real-time training visualization and 
professional CAD export capabilities.

Features:
    - Real-time epoch monitoring with progress bars
    - CUDA acceleration for RTX GPUs
    - Professional SVG and JSON export formats
    - Comprehensive training statistics
    - Optimized tensor operations

Author: TAP Development Team
Date: August 2025
License: MIT
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
import time
import sys
import os
from typing import Tuple, List, Dict, Optional, Any
from tqdm import tqdm
import json
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Add src to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), 'src')))

def setup_cuda_environment() -> torch.device:
    """
    Setup CUDA environment and return the optimal device.
    
    Returns:
        torch.device: CUDA device if available, otherwise CPU
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    if torch.cuda.is_available():
        torch.cuda.empty_cache()  # Clear any cached memory
        logger.info(f"CUDA device initialized: {torch.cuda.get_device_name(0)}")
    else:
        logger.warning("CUDA not available, falling back to CPU")
    return device

class ClassicalGenerator(nn.Module):
    """
    Classical generator network for geometric design generation.
    
    This generator uses a deep neural network with batch normalization
    and LeakyReLU activations to transform random noise into geometric
    design parameters.
    
    Args:
        latent_dim (int): Dimension of the input noise vector
        output_dim (int): Dimension of the output design vector
    """
    
    def __init__(self, latent_dim: int = 100, output_dim: int = 8) -> None:
        super(ClassicalGenerator, self).__init__()
        self.latent_dim = latent_dim
        self.output_dim = output_dim
        
        self.network = nn.Sequential(
            # Input layer
            nn.Linear(latent_dim, 256),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(256),
            
            # Hidden layers
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(512),
            
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(256),
            
            # Output layer
            nn.Linear(256, output_dim),
            nn.Tanh()  # Output in [-1, 1] range
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass through the generator.
        
        Args:
            x (torch.Tensor): Input noise tensor of shape (batch_size, latent_dim)
            
        Returns:
            torch.Tensor: Generated design parameters of shape (batch_size, output_dim)
        """
        return self.network(x)

class ClassicalDiscriminator(nn.Module):
    """
    Classical discriminator network for geometric design evaluation.
    
    This discriminator determines whether input geometric designs are
    real (from training data) or fake (generated by the generator).
    
    Args:
        input_dim (int): Dimension of the input design vector
    """
    
    def __init__(self, input_dim: int = 8) -> None:
        super(ClassicalDiscriminator, self).__init__()
        
        self.network = nn.Sequential(
            # Input layer
            nn.Linear(input_dim, 256),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            
            # Hidden layers
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            
            # Output layer
            nn.Linear(256, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass through the discriminator.
        
        Args:
            x (torch.Tensor): Input design tensor of shape (batch_size, input_dim)
            
        Returns:
            torch.Tensor: Probability that input is real, shape (batch_size, 1)
        """
        return self.network(x)

class ClassicalGAN:
    """
    Professional Classical GAN implementation for geometric design generation.
    
    This class manages the complete GAN training pipeline including:
    - Generator and discriminator networks
    - Training optimization
    - Loss tracking and visualization
    - Sample generation
    
    Args:
        latent_dim (int): Dimension of generator input noise
        output_dim (int): Dimension of generated design vectors
        device (Optional[torch.device]): Computing device (CUDA/CPU)
        learning_rate (float): Learning rate for optimizers
        betas (Tuple[float, float]): Beta parameters for Adam optimizer
    """
    
    def __init__(
        self, 
        latent_dim: int = 100, 
        output_dim: int = 8, 
        device: Optional[torch.device] = None,
        learning_rate: float = 0.0002,
        betas: Tuple[float, float] = (0.5, 0.999)
    ) -> None:
        self.latent_dim = latent_dim
        self.output_dim = output_dim
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Initialize networks
        self.generator = ClassicalGenerator(latent_dim, output_dim).to(self.device)
        self.discriminator = ClassicalDiscriminator(output_dim).to(self.device)
        
        # Loss function
        self.criterion = nn.BCELoss()
        
        # Optimizers with configurable parameters
        self.g_optimizer = optim.Adam(
            self.generator.parameters(), 
            lr=learning_rate, 
            betas=betas
        )
        self.d_optimizer = optim.Adam(
            self.discriminator.parameters(), 
            lr=learning_rate, 
            betas=betas
        )
        
        # Training history
        self.g_losses: List[float] = []
        self.d_losses: List[float] = []
        
        logger.info(f"ClassicalGAN initialized on device: {self.device}")
    
    def generate_noise(self, batch_size: int) -> torch.Tensor:
        """
        Generate random noise for the generator input.
        
        Args:
            batch_size (int): Number of noise vectors to generate
            
        Returns:
            torch.Tensor: Random noise tensor of shape (batch_size, latent_dim)
        """
        return torch.randn(batch_size, self.latent_dim, device=self.device)
    
    def train_step(self, real_data: torch.Tensor) -> Tuple[float, float]:
        """
        Perform one training step for both generator and discriminator.
        
        Args:
            real_data (torch.Tensor): Batch of real geometric designs
            
        Returns:
            Tuple[float, float]: (discriminator_loss, generator_loss)
        """
        batch_size = real_data.size(0)
        
        # Create labels
        real_labels = torch.ones(batch_size, 1, device=self.device)
        fake_labels = torch.zeros(batch_size, 1, device=self.device)
        
        # Train Discriminator
        self.d_optimizer.zero_grad()
        
        # Real data forward pass
        real_output = self.discriminator(real_data)
        d_loss_real = self.criterion(real_output, real_labels)
        
        # Fake data forward pass
        noise = self.generate_noise(batch_size)
        fake_data = self.generator(noise)
        fake_output = self.discriminator(fake_data.detach())
        d_loss_fake = self.criterion(fake_output, fake_labels)
        
        # Total discriminator loss and backward pass
        d_loss = d_loss_real + d_loss_fake
        d_loss.backward()
        self.d_optimizer.step()
        
        # Train Generator
        self.g_optimizer.zero_grad()
        
        # Generate fake data and try to fool discriminator
        noise = self.generate_noise(batch_size)
        fake_data = self.generator(noise)
        fake_output = self.discriminator(fake_data)
        g_loss = self.criterion(fake_output, real_labels)
        
        g_loss.backward()
        self.g_optimizer.step()
        
        return d_loss.item(), g_loss.item()
    
    def generate_samples(self, num_samples: int) -> torch.Tensor:
        """
        Generate samples from the trained generator.
        
        Args:
            num_samples (int): Number of samples to generate
            
        Returns:
            torch.Tensor: Generated design samples
        """
        self.generator.eval()
        with torch.no_grad():
            noise = self.generate_noise(num_samples)
            samples = self.generator(noise)
        self.generator.train()
        return samples

def create_synthetic_geometric_data(num_samples: int = 1000, output_dim: int = 8) -> torch.Tensor:
    """
    Create synthetic geometric data for training the GAN.
    
    This function generates structured geometric patterns that simulate
    real geometric design data with various pattern types.
    
    Args:
        num_samples (int): Number of geometric samples to generate
        output_dim (int): Dimension of each geometric design vector
        
    Returns:
        torch.Tensor: Synthetic geometric data tensor of shape (num_samples, output_dim)
    """
    logger.info(f"Creating {num_samples} synthetic geometric designs...")
    
    # Pre-allocate array for better performance
    data = np.zeros((num_samples, output_dim), dtype=np.float32)
    
    for i in range(num_samples):
        # Generate different geometric pattern types
        pattern_type = i % 4
        
        if pattern_type == 0:  # Circular patterns
            t = np.linspace(0, 2*np.pi, output_dim//2) + np.random.random()
            data[i, 0::2] = np.sin(t) + 0.1 * np.random.randn(output_dim//2)
            data[i, 1::2] = np.cos(t) + 0.1 * np.random.randn(output_dim//2)
            
        elif pattern_type == 1:  # Linear patterns
            t = np.linspace(-1, 1, output_dim)
            data[i] = t + 0.2 * np.random.randn(output_dim)
            
        elif pattern_type == 2:  # Sinusoidal patterns
            t = np.linspace(0, 2*np.pi, output_dim)
            data[i] = np.sin(t + np.random.random() * 2*np.pi) + 0.1 * np.random.randn(output_dim)
            
        else:  # Random geometric patterns
            data[i] = np.random.uniform(-1, 1, output_dim)
    
    # Convert to tensor efficiently
    return torch.from_numpy(data)

def train_classical_gan(
    epochs: int = 100, 
    batch_size: int = 32, 
    save_interval: int = 10
) -> Tuple[ClassicalGAN, torch.Tensor]:
    """
    Train the classical GAN with real-time epoch display.
    
    Args:
        epochs (int): Number of training epochs
        batch_size (int): Batch size for training
        save_interval (int): Interval for saving sample outputs
        
    Returns:
        Tuple[ClassicalGAN, torch.Tensor]: Trained GAN and training data
    """
    
    print("üöÄ CLASSICAL GAN TRAINING WITH REAL-TIME DISPLAY")
    print("=" * 70)
    
    # Setup device
    device = setup_cuda_environment()
    print(f"Using device: {device}")
    
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    
    # Create training data
    train_data = create_synthetic_geometric_data(2000, output_dim=8)
    
    # Create dataset and dataloader
    class GeometricDataset(torch.utils.data.Dataset):
        def __init__(self, data):
            self.data = data
        
        def __len__(self):
            return len(self.data)
        
        def __getitem__(self, idx):
            return self.data[idx]
    
    dataset = GeometricDataset(train_data)
    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    # Initialize GAN
    gan = ClassicalGAN(latent_dim=100, output_dim=8, device=device)
    
    print(f"\nTraining for {epochs} epochs...")
    print(f"Batch size: {batch_size}")
    print(f"Batches per epoch: {len(train_loader)}")
    print("=" * 70)
    
    # Training loop with real-time display
    start_time = time.time()
    
    for epoch in range(epochs):
        epoch_start_time = time.time()
        epoch_d_losses = []
        epoch_g_losses = []
        
        # Progress bar for batches
        batch_progress = tqdm(train_loader, desc=f"Epoch {epoch+1:3d}/{epochs}", 
                            leave=False, ncols=80)
        
        for batch_idx, real_batch in enumerate(batch_progress):
            real_batch = real_batch.to(device)
            
            # Train step
            d_loss, g_loss = gan.train_step(real_batch)
            epoch_d_losses.append(d_loss)
            epoch_g_losses.append(g_loss)
            
            # Update progress bar
            batch_progress.set_postfix({
                'D_loss': f'{d_loss:.4f}',
                'G_loss': f'{g_loss:.4f}'
            })
        
        # Calculate epoch averages
        avg_d_loss = np.mean(epoch_d_losses)
        avg_g_loss = np.mean(epoch_g_losses)
        epoch_time = time.time() - epoch_start_time
        
        # Store losses with proper type conversion
        gan.d_losses.append(float(avg_d_loss))
        gan.g_losses.append(float(avg_g_loss))
        
        # Real-time epoch display
        elapsed_time = time.time() - start_time
        eta = (elapsed_time / (epoch + 1)) * (epochs - epoch - 1)
        
        print(f"Epoch {epoch+1:3d}/{epochs} | "
              f"D_loss: {avg_d_loss:.4f} | "
              f"G_loss: {avg_g_loss:.4f} | "
              f"Time: {epoch_time:.2f}s | "
              f"ETA: {eta:.0f}s")
        
        # Save samples periodically
        if (epoch + 1) % save_interval == 0:
            with torch.no_grad():
                samples = gan.generate_samples(5)
                print(f"  Generated sample range: [{samples.min():.3f}, {samples.max():.3f}]")
    
    total_time = time.time() - start_time
    print("=" * 70)
    print(f"‚úÖ Training completed in {total_time:.2f} seconds")
    print(f"Final D_loss: {gan.d_losses[-1]:.4f}")
    print(f"Final G_loss: {gan.g_losses[-1]:.4f}")
    
    return gan, train_data

def generate_and_export_designs(gan: ClassicalGAN, num_designs: int = 10) -> List[Dict[str, Any]]:
    """
    Generate geometric designs and export to CAD-friendly formats.
    
    Args:
        gan (ClassicalGAN): Trained GAN model
        num_designs (int): Number of designs to generate
        
    Returns:
        List[Dict[str, Any]]: List of geometric design data dictionaries
    """
    logger.info("üìê GENERATING AND EXPORTING DESIGNS")
    logger.info("=" * 50)
    
    # Generate samples
    samples = gan.generate_samples(num_designs)
    logger.info(f"Generated {num_designs} geometric designs")
    logger.info(f"Sample shape: {samples.shape}")
    
    # Convert to numpy for processing
    designs = samples.cpu().numpy()
    
    # Create geometric interpretations
    geometric_data: List[Dict[str, Any]] = []
    for i, design in enumerate(designs):
        # Interpret the 8D vector as 4 2D points (quadrilateral)
        points = design.reshape(4, 2)
        geometric_data.append({
            'id': f'design_{i+1}',
            'type': 'quadrilateral',
            'points': points.tolist(),
            'bounds': {
                'min_x': float(points[:, 0].min()),
                'max_x': float(points[:, 0].max()),
                'min_y': float(points[:, 1].min()),
                'max_y': float(points[:, 1].max())
            }
        })
    
    # Export to files
    try:
        # JSON export with proper formatting
        with open('classical_designs.json', 'w', encoding='utf-8') as f:
            json.dump(geometric_data, f, indent=2, ensure_ascii=False)
        logger.info("‚úÖ Exported to: classical_designs.json")
        
        # SVG export
        svg_content = create_professional_svg(geometric_data)
        with open('classical_designs.svg', 'w', encoding='utf-8') as f:
            f.write(svg_content)
        logger.info("‚úÖ Exported to: classical_designs.svg")
        
    except Exception as e:
        logger.error(f"Export error: {e}")
    
    return geometric_data

def create_professional_svg(geometric_data: List[Dict[str, Any]]) -> str:
    """
    Create a professional SVG representation of geometric data.
    
    Args:
        geometric_data (List[Dict[str, Any]]): List of geometric design dictionaries
        
    Returns:
        str: SVG content as string
    """
    svg_lines = [
        '<?xml version="1.0" encoding="UTF-8"?>',
        '<svg width="800" height="600" xmlns="http://www.w3.org/2000/svg">',
        '<defs>',
        '  <style>',
        '    .design-shape { stroke-width: 2; stroke-linejoin: round; }',
        '    .background { fill: #f8f9fa; }',
        '    .title { font-family: Arial, sans-serif; font-size: 16px; font-weight: bold; }',
        '  </style>',
        '</defs>',
        '<rect width="100%" height="100%" class="background"/>',
        '<text x="20" y="30" class="title">Classical GAN Generated Geometric Designs</text>',
    ]
    
    # Professional color palette
    colors = [
        '#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6',
        '#1abc9c', '#e67e22', '#34495e', '#e91e63', '#00bcd4'
    ]
    
    # Calculate layout parameters
    canvas_width, canvas_height = 760, 540
    margin = 20
    start_y = 60
    
    for i, design in enumerate(geometric_data):
        points = design['points']
        color = colors[i % len(colors)]
        
        # Scale and position points within canvas
        scaled_points = []
        for x, y in points:
            # Scale from [-1,1] to canvas coordinates
            svg_x = margin + (x + 1) * canvas_width / 2
            svg_y = start_y + (y + 1) * canvas_height / 2
            scaled_points.append(f"{svg_x:.2f},{svg_y:.2f}")
        
        # Create polygon with professional styling
        points_str = " ".join(scaled_points)
        svg_lines.append(
            f'<polygon points="{points_str}" '
            f'fill="{color}" fill-opacity="0.6" '
            f'stroke="{color}" class="design-shape" '
            f'title="Design {i+1}"/>'
        )
        
        # Add design ID label
        center_x = margin + canvas_width / 2
        center_y = start_y + (i + 1) * 30
        svg_lines.append(
            f'<text x="20" y="{start_y + canvas_height + 30 + i*20}" '
            f'font-family="Arial, sans-serif" font-size="12" fill="{color}">‚ñ† {design["id"]}</text>'
        )
    
    svg_lines.append('</svg>')
    return '\n'.join(svg_lines)

def plot_training_progress(gan: ClassicalGAN) -> None:
    """
    Plot and save training loss curves with professional styling.
    
    Args:
        gan (ClassicalGAN): Trained GAN with loss history
    """
    logger.info("üìä PLOTTING TRAINING PROGRESS")
    logger.info("=" * 40)
    
    try:
        # Create professional-looking plots
        plt.style.use('default')
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        fig.suptitle('Classical GAN Training Progress', fontsize=16, fontweight='bold')
        
        # Raw losses plot
        epochs = range(1, len(gan.d_losses) + 1)
        ax1.plot(epochs, gan.d_losses, label='Discriminator Loss', 
                color='#e74c3c', alpha=0.8, linewidth=2)
        ax1.plot(epochs, gan.g_losses, label='Generator Loss', 
                color='#3498db', alpha=0.8, linewidth=2)
        ax1.set_xlabel('Epoch', fontsize=12)
        ax1.set_ylabel('Loss', fontsize=12)
        ax1.set_title('Training Losses', fontsize=14, fontweight='bold')
        ax1.legend(fontsize=11)
        ax1.grid(True, alpha=0.3)
        ax1.set_facecolor('#f8f9fa')
        
        # Smoothed losses plot (if enough data)
        if len(gan.d_losses) > 10:
            window = max(3, len(gan.d_losses) // 10)
            d_smooth = np.convolve(gan.d_losses, np.ones(window)/window, mode='valid')
            g_smooth = np.convolve(gan.g_losses, np.ones(window)/window, mode='valid')
            smooth_epochs = range(window, len(gan.d_losses) + 1)
            
            ax2.plot(smooth_epochs, d_smooth, label='D Loss (Smoothed)', 
                    color='#c0392b', linewidth=3)
            ax2.plot(smooth_epochs, g_smooth, label='G Loss (Smoothed)', 
                    color='#2980b9', linewidth=3)
        else:
            # Fallback for small datasets
            ax2.plot(epochs, gan.d_losses, label='Discriminator Loss', 
                    color='#e74c3c', linewidth=2)
            ax2.plot(epochs, gan.g_losses, label='Generator Loss', 
                    color='#3498db', linewidth=2)
        
        ax2.set_xlabel('Epoch', fontsize=12)
        ax2.set_ylabel('Loss', fontsize=12)
        ax2.set_title('Smoothed Training Losses', fontsize=14, fontweight='bold')
        ax2.legend(fontsize=11)
        ax2.grid(True, alpha=0.3)
        ax2.set_facecolor('#f8f9fa')
        
        plt.tight_layout()
        plt.savefig('training_progress.png', dpi=300, bbox_inches='tight', 
                   facecolor='white', edgecolor='none')
        plt.show()
        
        logger.info("‚úÖ Training progress saved to: training_progress.png")
        
    except Exception as e:
        logger.error(f"Plotting error: {e}")
        # Fallback to simple text output
        logger.info("Fallback: Printing loss statistics")
        logger.info(f"Final D_loss: {gan.d_losses[-1]:.4f}")
        logger.info(f"Final G_loss: {gan.g_losses[-1]:.4f}")
        logger.info(f"Average D_loss: {np.mean(gan.d_losses):.4f}")
        logger.info(f"Average G_loss: {np.mean(gan.g_losses):.4f}")

def main() -> None:
    """
    Main execution function for the Classical GAN system.
    
    This function orchestrates the complete training pipeline:
    1. System initialization and CUDA setup
    2. GAN training with real-time monitoring
    3. Design generation and export
    4. Training visualization
    """
    logger.info("üéØ CLASSICAL GAN FOR GEOMETRIC DESIGN GENERATION")
    logger.info("Real-time epoch display, CUDA acceleration, CAD export")
    logger.info("=" * 70)
    
    try:
        # Train the GAN with professional monitoring
        gan, train_data = train_classical_gan(epochs=50, batch_size=32, save_interval=5)
        
        # Generate and export professional designs
        designs = generate_and_export_designs(gan, num_designs=10)
        
        # Create professional training visualizations
        plot_training_progress(gan)
        
        # Success summary
        logger.info("\nüéâ CLASSICAL GAN TRAINING COMPLETED SUCCESSFULLY!")
        logger.info("=" * 70)
        logger.info("Generated files:")
        logger.info("  ‚Ä¢ classical_designs.json - Structured geometric data")
        logger.info("  ‚Ä¢ classical_designs.svg - Professional vector graphics")
        logger.info("  ‚Ä¢ training_progress.png - High-resolution loss curves")
        logger.info("\nüöÄ Ready for CAD import and professional use!")
        
        # Performance summary
        final_d_loss = gan.d_losses[-1] if gan.d_losses else 0
        final_g_loss = gan.g_losses[-1] if gan.g_losses else 0
        logger.info(f"\nFinal Performance:")
        logger.info(f"  Discriminator Loss: {final_d_loss:.4f}")
        logger.info(f"  Generator Loss: {final_g_loss:.4f}")
        logger.info(f"  Generated Designs: {len(designs)}")
        
    except KeyboardInterrupt:
        logger.warning("\n‚èπÔ∏è  Training interrupted by user")
        logger.info("Partial results may be available in output files")
    except Exception as e:
        logger.error(f"\n‚ùå Unexpected error: {e}")
        logger.error("Please check your environment and dependencies")
        import traceback
        logger.debug(traceback.format_exc())

if __name__ == "__main__":
    main()
